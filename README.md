# The pipeline to estimate maximum growth rate of bacteria from ASVs

**STEPS**:

1. Fetch genomes for ASVs matches:

    `sbatch sub_getgenome.bash`

    This command line calls `getgenome_parallel.sh` with the input file `test.tsv` to get full genomes sequences for all genome ids, e.g. GB_GCA_905618805.1 in `test.tsv` file. `test.tsv` is the BALST output by comparing ASVs with the databases.

2. Annotate genomes downloaded by Step 1:

    `sbatch sub_prokkagenome.bash`

    This command line calls `prokka_genomes.sh` with the input file `test.tsv` to annotate the genomes. The genome ids are the input arguments. Note that the file `221201_P16N-S.prok-nonphoautototrophic.BLAST-95pcID-vs-GTDB-r207-allproks.tsv` is in the same format as the file `test.tsv`. The original data is not provided in this pipeline. Please replace the file with your own data. One additional file providing the information of `kingdom` is needed. This information tells `prokka` what taxonomy the genome is. The file name `SPOT_Prokaryotic16S_ASV_Domain_identity.csv` should be replaced with the users' file in prokka_genomes_SPOT.sh 

3. Generate CDS name files and estimate max growth rates:

    `sbatch sub_gRodon_genomes.bash`

    This command line calls `R_CDS.sh` to generate CDS_names.txt first and then run `gRodonGenomes.R` in **R** to estimate the max growth rates for each genome. The unique genome ids are stored in `genomeid.tsv`, which is pulled out from the ASVs contrast file, e.g. `test.tsv`, using the script `Hist_hits.py` (the second part) in **Python**. The estimation script will take `XXX_.ffn` and `XXX_CDS_names.txt` as the input for the `gRodon2` function. The estimation results will be stored in `XXX_growth_est.json` in the same folder as the annotation results. 

## Get genome fasta files from a tsv file generated by BLAST

This is a way to get genome data by genome id from a tsv file generated by BLAST. The tsv file contains information of the species id and the genomen hit ids. One example is provided in the repo. `sh` scripting is used to automate the downloading processes facilitated by the two tools `datasets` and `dataformat` from NCBI. The **parallel fetching** has been implemented. A job submission file is provided for the cluster usage.

### Install `datasets` and `dataformat` from NCBI

Follow the [link](https://www.ncbi.nlm.nih.gov/datasets/docs/v2/download-and-install/) to install these two tools. 


### Run `getgenome_parallel_uniformdb.sh`

Basically, what this script does is:

1. Extract the species label, e.g. c497da3b39f30aceede6bec3b03cd100 and the genome id, e.g. GB_GCA_905618805.1 from each line in the tsv file. One species label may have several genome ids, also called hits;

2. Generate folders for each genome id; Different ASVs may have the same match with the same genome. The repeated matched genome is downloaed once;

3. Run `datasets` to download the genome data package. Make sure `datasets` is in the same folder as the `sh` script. 

```
sh getgenome_parallel_uniformdb.sh
```

The package contains a bunch to information. See [here](https://www.ncbi.nlm.nih.gov/datasets/docs/v2/reference-docs/data-packages/genome/) for details. A fasta file is included in the package;

4. Unzip the downloaded data. 


### Run `getgenome_parallel.sh -number-of-cores -filename.tsv`

Basically, what this script does is as the same as `getgenome.sh`. The parallel processing is implemented via [GNU `parallel`](https://www.gnu.org/software/parallel/). Note that two arguments are needed to run this `sh` as follows, taking MacOS terminal as an example:

```bash
sh getgenome_parallel.sh -number-of-cores -filename.tsv
```

### Annotating genomes using [Prokka](https://github.com/tseemann/prokka)

The script is `prokka_genomes.sh`

It reads the genomes sequences `.fasta` file and annotates and output a bunch of files. See  [Prokka](https://github.com/tseemann/prokka) for more details.

Singluarity image of Prokka is used on Caltech HPC to install the package. 

The output files will be stored in "prokkaoutput" folder. The genome annotations will be stored in the folder named after the genome. 


### Caveats

1. Being tested on Linux. Running well on the Caltech cluster;


### Future work

1. Using [gRodon](https://github.com/jlw-ecoevo/gRodon2) to estimate the maximum growth rates.

## Some useful numbers

1. Annotating one full genome using Prokka takes about 6 minutes on the Caltech HPC cluster;

2. Estimating max growth rates using gRodon2 for one genome takes about 1 minute on the Caltech HPC cluster.


## Historical scripts; **deprecated now**

### Run `getgenome.sh`

Basically, what this script does is:

1. Extract the species label, e.g. c497da3b39f30aceede6bec3b03cd100 and the genome id, e.g. GB_GCA_905618805.1 from each line in the tsv file. One species label may have several genome ids, also called hits;

2. Generate folders for species and the corresponding hits;

3. Run `datasets` to download the genome data package. Make sure `datasets` is in the same folder as the `sh` script. 

```
sh getgenome.sh
```

The package contains a bunch to information. See [here](https://www.ncbi.nlm.nih.gov/datasets/docs/v2/reference-docs/data-packages/genome/) for details. A fasta file is included in the package;

4. Unzip the downloaded data. 


### Run `getgenome_parallel.sh -number-of-cores -filename.tsv`

Basically, what this script does is as the same as `getgenome.sh`. The parallel processing is implemented via [GNU `parallel`](https://www.gnu.org/software/parallel/). Note that two arguments are needed to run this `sh` as follows, taking MacOS terminal as an example:

```bash
sh getgenome_parallel.sh -number-of-cores -filename.tsv
```


